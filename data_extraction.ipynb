{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "mount_file_id": "1kA77ttlL2vjY4Hr1Tmz3clAr2gzSrBmt",
      "authorship_tag": "ABX9TyNfkdb3bzTm/n4Ar7ei4rMK",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/shairal1/Knowledge_graph_rag/blob/main/data_extraction.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# CSX / extended ASCII mappings\n",
        "csx_to_iast = {\n",
        "    \"à\": \"ā\",  \"â\": \"Ā\",\n",
        "    \"ã\": \"ī\",  \"ä\": \"Ī\",\n",
        "    \"å\": \"ū\",  \"æ\": \"Ū\",\n",
        "    \"ç\": \"ṛ\",  \"è\": \"Ṛ\",  \"é\": \"ṝ\",\n",
        "    \"ë\": \"ḷ\",  \"í\": \"ḹ\",\n",
        "    \"ï\": \"ṅ\",  \"ð\": \"Ṅ\",\n",
        "    \"¤\": \"ñ\",  \"¥\": \"Ñ\",\n",
        "    \"ñ\": \"ṭ\",  \"ò\": \"Ṭ\",\n",
        "    \"ó\": \"ḍ\",  \"ô\": \"Ḍ\",\n",
        "    \"õ\": \"ṇ\",  \"ö\": \"Ṇ\",\n",
        "    \"÷\": \"ś\",  \"ø\": \"Ś\",\n",
        "    \"ù\": \"ṣ\",  \"ú\": \"Ṣ\",\n",
        "    \"ü\": \"ṃ\",  \"ý\": \"Ṃ\",\n",
        "    \"þ\": \"ḥ\",\n",
        "    \"¹\": \"ē\",  \"º\": \"ō\",\n",
        "    \"×\": \"ḷ\",  \"Ÿ\": \"ṛ\",\n",
        "    \"­\": \"ṇ\",  \"É\": \"ḵ\",  \"Â\": \"ṭ̱\"\n",
        "}\n",
        "\n",
        "# Under-dot ASCII mappings\n",
        "underdot_to_iast = {\n",
        "    \"r.\": \"ṛ\",\n",
        "    \"r..\": \"ṝ\",\n",
        "    \"l.\": \"ḷ\",\n",
        "    \"l..\": \"ḹ\",\n",
        "    \"n.\": \"ṇ\",\n",
        "    \"t.\": \"ṭ\",\n",
        "    \"d.\": \"ḍ\",\n",
        "    \"s.\": \"ṣ\",\n",
        "    \"m.\": \"ṃ\",\n",
        "    \"h.\": \"ḥ\"\n",
        "}\n",
        "\n",
        "# Merge mappings for general use\n",
        "def build_full_mapping():\n",
        "    full_map = csx_to_iast.copy()\n",
        "    # add under-dot ASCII mapping (single character replacement only)\n",
        "    for k, v in underdot_to_iast.items():\n",
        "        # treat 'r.' etc as two-char sequences\n",
        "        full_map[k] = v\n",
        "    return full_map\n",
        "\n",
        "full_mapping = build_full_mapping()\n",
        "import unicodedata\n",
        "import re\n",
        "\n",
        "def normalize_sanskrit(text):\n",
        "    # Step 1: replace CSX / extended ASCII\n",
        "    for k, v in csx_to_iast.items():\n",
        "        text = text.replace(k, v)\n",
        "\n",
        "    # Step 2: replace under-dot ASCII sequences (longest first)\n",
        "    for k, v in sorted(underdot_to_iast.items(), key=lambda x: -len(x[0])):\n",
        "        text = text.replace(k, v)\n",
        "\n",
        "    # Step 3: normalize Unicode\n",
        "    text = unicodedata.normalize('NFC', text)\n",
        "\n",
        "    # Step 4: optional cleanup (extra spaces)\n",
        "    text = re.sub(r'\\s+', ' ', text).strip()\n",
        "\n",
        "    return text\n"
      ],
      "metadata": {
        "id": "DO6eIEtLRadA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import requests\n",
        "\n",
        "# URL of the text file\n",
        "url = \"https://gretil.sub.uni-goettingen.de/gretil/1_sanskr/2_epic/mbh/ext/bhg4c__c.txt\"\n",
        "\n",
        "# Path where you want to save the file\n",
        "# Example Windows: \"C:/Users/YourName/Documents/bhg4c__c.txt\"\n",
        "# Example Mac/Linux: \"/Users/YourName/Documents/bhg4c__c.txt\"\n",
        "save_path = \"/content/bhg4c__c.txt\"\n",
        "\n",
        "# Fetch the file\n",
        "response = requests.get(url)\n",
        "\n",
        "if response.status_code == 200:\n",
        "    with open(save_path, \"w\", encoding=\"utf-8\") as f:\n",
        "        f.write(response.text)\n",
        "    print(f\"File saved successfully at {save_path}\")\n",
        "else:\n",
        "    print(f\"Failed to download. Status code: {response.status_code}\")\n",
        "with open('/content/bhg4c__c.txt', 'r', encoding='utf-8') as f:\n",
        "    content = f.read()  # Reads the entire file into a string\n",
        "\n",
        "# Print first 500 characters to check\n",
        "print(content[:500])\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "B1bC_Q52AtLg",
        "outputId": "6a12a0e2-9a4e-43b1-da75-0d90ad9e1b15"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Bhagavadgita\n",
            "with the commentaries of Sridhara, (Madhusudana), Visvanatha and Baladeva\n",
            "\n",
            "Input by ... (Gaudiya Grantha Mandira)\n",
            "\n",
            "\n",
            "___________________________________________________________________\n",
            "\n",
            "THIS TEXT FILE IS FOR REFERENCE PURPOSES ONLY!   \n",
            "COPYRIGHT AND TERMS OF USAGE AS FOR SOURCE FILE. \n",
            "                                       \n",
            "Text converted to Classical Sanskrit Extended (CSX) encoding:\n",
            "                                       \n",
            "description             character  =ASCII   \n",
            "               \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from os import path\n",
        "import json\n",
        "import os\n",
        "\n",
        "def gretil(output_dir):\n",
        "  output_file = os.path.join(output_dir, \"gretil_extracted_BHG.json\")\n",
        "  clean_text = \"\\n\".join(line for line in content.splitlines() if not line.startswith(\"[*ENDNOTE]\"))\n",
        "  clean_text = normalize_sanskrit(clean_text)\n",
        "\n",
        "  chunk_split = re.split(r'(BhG \\d+\\.\\d+)', clean_text)\n",
        "  chunk_split.pop(0)\n",
        "  verse_id=[]\n",
        "  text=[]\n",
        "  for i in range(0,len(chunk_split),2):\n",
        "    verse_id.append((chunk_split[i]).strip())\n",
        "    text.append(chunk_split[i+1].strip())\n",
        "  nested_json = {}\n",
        "  pattern = r'.*?\\|\\|\\d+\\|\\|'\n",
        "  for i in range(len(text)):\n",
        "      chapter_number=verse_id[i]\n",
        "      nested_json[chapter_number] = {\"chapter\": chapter_number, \"verse\": [],\"commentaries\":[]}\n",
        "      split=re.findall(pattern, text[i], flags=re.DOTALL)\n",
        "      nested_json[chapter_number][\"verse\"].append(split[0])\n",
        "      nested_json[chapter_number][\"commentaries\"].append(split[1:])\n",
        "\n",
        "  with open(f'{output_file}', \"w\", encoding=\"utf-8\") as f:\n",
        "    json.dump(nested_json, f, ensure_ascii=False, indent=4)\n",
        "\n",
        "gretil(\"/content/drive/MyDrive/Project\")"
      ],
      "metadata": {
        "id": "QU1DuB3NMecG"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "########### Book\n"
      ],
      "metadata": {
        "id": "iHK0ZeAYSvE_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install pdfplumber"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9Yvt2ZHtS3zA",
        "outputId": "ec925a11-f785-4711-acda-c2593b972492"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting pdfplumber\n",
            "  Downloading pdfplumber-0.11.7-py3-none-any.whl.metadata (42 kB)\n",
            "\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/42.8 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m42.8/42.8 kB\u001b[0m \u001b[31m1.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting pdfminer.six==20250506 (from pdfplumber)\n",
            "  Downloading pdfminer_six-20250506-py3-none-any.whl.metadata (4.2 kB)\n",
            "Requirement already satisfied: Pillow>=9.1 in /usr/local/lib/python3.12/dist-packages (from pdfplumber) (11.3.0)\n",
            "Collecting pypdfium2>=4.18.0 (from pdfplumber)\n",
            "  Downloading pypdfium2-4.30.0-py3-none-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (48 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m48.5/48.5 kB\u001b[0m \u001b[31m4.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: charset-normalizer>=2.0.0 in /usr/local/lib/python3.12/dist-packages (from pdfminer.six==20250506->pdfplumber) (3.4.3)\n",
            "Requirement already satisfied: cryptography>=36.0.0 in /usr/local/lib/python3.12/dist-packages (from pdfminer.six==20250506->pdfplumber) (43.0.3)\n",
            "Requirement already satisfied: cffi>=1.12 in /usr/local/lib/python3.12/dist-packages (from cryptography>=36.0.0->pdfminer.six==20250506->pdfplumber) (2.0.0)\n",
            "Requirement already satisfied: pycparser in /usr/local/lib/python3.12/dist-packages (from cffi>=1.12->cryptography>=36.0.0->pdfminer.six==20250506->pdfplumber) (2.23)\n",
            "Downloading pdfplumber-0.11.7-py3-none-any.whl (60 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m60.0/60.0 kB\u001b[0m \u001b[31m5.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading pdfminer_six-20250506-py3-none-any.whl (5.6 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.6/5.6 MB\u001b[0m \u001b[31m49.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading pypdfium2-4.30.0-py3-none-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (2.8 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.8/2.8 MB\u001b[0m \u001b[31m60.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: pypdfium2, pdfminer.six, pdfplumber\n",
            "Successfully installed pdfminer.six-20250506 pdfplumber-0.11.7 pypdfium2-4.30.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pdfplumber\n",
        "\n",
        "pdf_path = \"/content/Bhagavad-gita-As-It-Is_repaired.pdf\"\n",
        "pdf_text = \"\"\n",
        "\n",
        "with pdfplumber.open(pdf_path) as pdf:\n",
        "    for page in pdf.pages:\n",
        "        pdf_text += page.extract_text() + \"\\n\""
      ],
      "metadata": {
        "id": "QQ2Oy3HCS2iB"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def clean_text(text: str) -> str:\n",
        "    # Replace page breaks (\\f), carriage returns (\\r), multiple newlines, tabs\n",
        "    text = text.replace('\\f', ' ')       # page breaks\n",
        "    text = text.replace('\\r', ' ')       # carriage returns\n",
        "    text = text.replace('\\t', ' ')\n",
        "    return text\n",
        "\n",
        "entries = []\n",
        "import re\n",
        "\n",
        "def split_transliteration_and_meaning(block: str):\n",
        "    lines = block.strip().split(\"\\n\")\n",
        "    translit_lines = []\n",
        "    meaning_lines = []\n",
        "\n",
        "    for line in lines:\n",
        "        # If line contains a word-gloss like \"word—meaning\"\n",
        "        if re.search(r'\\w+—', line):\n",
        "            meaning_lines.append(line)\n",
        "        else:\n",
        "            translit_lines.append(line)\n",
        "\n",
        "    transliteration = \"\\n\".join(translit_lines).strip()\n",
        "    meaning = \"\\n\".join(meaning_lines).strip()\n",
        "\n",
        "    return transliteration, meaning\n",
        "\n",
        "# Split by text number (assuming they appear as \"TEXT 22\", \"TEXT 23\", etc.)\n",
        "text_blocks = re.split(r'TEXT\\s+(\\d+)', pdf_text)\n",
        "\n",
        "# text_blocks[0] is before the first TEXT, so skip it\n",
        "for i in range(1, len(text_blocks), 2):\n",
        "    text_number = int(text_blocks[i].strip())\n",
        "    content = text_blocks[i+1].strip()\n",
        "\n",
        "    # Extract sections\n",
        "    # Assuming headers like \"TRANSLATION\" and \"PURPORT\" exist\n",
        "    headerparts = content.split(\"TRANSLATION\")[0].strip()\n",
        "    translation = re.search(r'TRANSLATION\\s*(.*?)(PURPORT|$)', content, re.DOTALL).group(1).strip()\n",
        "    purport_match = re.search(r'PURPORT\\s*(.*)', content, re.DOTALL)\n",
        "    purport = purport_match.group(1).strip() if purport_match else \"\"\n",
        "\n",
        "    headerparts = re.split(r'\\)\\)\\s*\\d+\\s*\\)\\)', headerparts)[1]\n",
        "    transliteration,word_meaning=split_transliteration_and_meaning(headerparts)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "    # Optional: extract transliteration if exists (custom logic depending on PDF)\n",
        "\n",
        "    entry = {\n",
        "        \"text_number\": text_number,\n",
        "        \"verse\": normalize_sanskrit(clean_text(transliteration)),\n",
        "        \"mean_word2word\": normalize_sanskrit(clean_text(word_meaning)),  # fill if you can parse it\n",
        "        \"translation\": normalize_sanskrit(clean_text(translation)),\n",
        "        \"purport\": normalize_sanskrit(clean_text(purport))\n",
        "    }\n",
        "\n",
        "    entries.append(entry)\n",
        "\n",
        "# Save to JSON"
      ],
      "metadata": {
        "id": "0DFMn7LHSpQV"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "with open(\"/content/drive/MyDrive/A_C_Bhaktivedanta_BhG.json\", \"w\", encoding=\"utf-8\") as f:\n",
        "    json.dump(entries, f, ensure_ascii=False, indent=4)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FqgrQdlaU89h",
        "outputId": "c6ed4654-03ed-4b55-f464-6a38f4abf8f9"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "621 entries saved to structured_texts.json\n"
          ]
        }
      ]
    }
  ]
}